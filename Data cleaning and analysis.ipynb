{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dfe0e17",
   "metadata": {},
   "source": [
    "# Assignment 2 Part B\n",
    "\n",
    "## Harmony Heals \n",
    "\n",
    "### Project by RAS (Group 45)\n",
    "- Rohan Britto (Student ID: 24610990)\n",
    "- Aaditya Deshmukh (Student ID: )\n",
    "- Smit Khatri (Student ID: )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec82ecf",
   "metadata": {},
   "source": [
    "## Importing packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3463442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9746b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/muse_v3_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89c3e0",
   "metadata": {},
   "source": [
    "Checking the features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "da2bc454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90001 entries, 0 to 90000\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   lastfm_url              90001 non-null  object \n",
      " 1   track                   90001 non-null  object \n",
      " 2   artist                  90001 non-null  object \n",
      " 3   seeds                   90001 non-null  object \n",
      " 4   number_of_emotion_tags  90001 non-null  int64  \n",
      " 5   valence_tags            90001 non-null  float64\n",
      " 6   arousal_tags            90001 non-null  float64\n",
      " 7   dominance_tags          90001 non-null  float64\n",
      " 8   mbid                    61217 non-null  object \n",
      " 9   spotify_id              61630 non-null  object \n",
      " 10  genre                   83362 non-null  object \n",
      " 11  lyrics                  90001 non-null  object \n",
      "dtypes: float64(3), int64(1), object(8)\n",
      "memory usage: 8.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc94685",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7fd4b",
   "metadata": {},
   "source": [
    "Let us check if there are any duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b9883414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cadcc1",
   "metadata": {},
   "source": [
    "Copying the dataframe so that it can be restored if there is any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3d1a8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.drop(['lastfm_url','seeds','spotify_id','genre','mbid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3697e25",
   "metadata": {},
   "source": [
    "We will use only those rows that have lyrics populated in them. Rows with null values, Not Found, Not English and Exception in lyrics column need to be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a5264050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[~df_cleaned['lyrics'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3ef3eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[df_cleaned['lyrics']!='Not Found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "08c3c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[df_cleaned['lyrics']!='Not English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1880f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned[df_cleaned['lyrics']!='Exception']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608fa53",
   "metadata": {},
   "source": [
    "A quick look at the data after clearing the columns that are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "29a359e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>number_of_emotion_tags</th>\n",
       "      <th>valence_tags</th>\n",
       "      <th>arousal_tags</th>\n",
       "      <th>dominance_tags</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>St. Anger</td>\n",
       "      <td>Metallica</td>\n",
       "      <td>8</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>5.833000</td>\n",
       "      <td>5.427250</td>\n",
       "      <td>St. Anger 'round my neck\\r\\nSt. Anger 'round m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speedin'</td>\n",
       "      <td>Rick Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>5.870000</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>Legendary, Runners, you know me, o ooooh, o oo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bamboo Banga</td>\n",
       "      <td>M.I.A.</td>\n",
       "      <td>13</td>\n",
       "      <td>6.555071</td>\n",
       "      <td>5.537214</td>\n",
       "      <td>5.691357</td>\n",
       "      <td>Road runner, road runner\\r\\nGoing hundred mile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die MF Die</td>\n",
       "      <td>Dope</td>\n",
       "      <td>7</td>\n",
       "      <td>3.771176</td>\n",
       "      <td>5.348235</td>\n",
       "      <td>5.441765</td>\n",
       "      <td>I don't need your forgiveness\\r\\nI don't need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Step Up</td>\n",
       "      <td>Drowning Pool</td>\n",
       "      <td>9</td>\n",
       "      <td>2.971389</td>\n",
       "      <td>5.537500</td>\n",
       "      <td>4.726389</td>\n",
       "      <td>One, two, three, go\\r\\n\\r\\nBroken\\r\\nYeah, you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          track         artist  number_of_emotion_tags  valence_tags  \\\n",
       "1     St. Anger      Metallica                       8      3.710000   \n",
       "2      Speedin'      Rick Ross                       1      3.080000   \n",
       "3  Bamboo Banga         M.I.A.                      13      6.555071   \n",
       "4    Die MF Die           Dope                       7      3.771176   \n",
       "5       Step Up  Drowning Pool                       9      2.971389   \n",
       "\n",
       "   arousal_tags  dominance_tags  \\\n",
       "1      5.833000        5.427250   \n",
       "2      5.870000        5.490000   \n",
       "3      5.537214        5.691357   \n",
       "4      5.348235        5.441765   \n",
       "5      5.537500        4.726389   \n",
       "\n",
       "                                              lyrics  \n",
       "1  St. Anger 'round my neck\\r\\nSt. Anger 'round m...  \n",
       "2  Legendary, Runners, you know me, o ooooh, o oo...  \n",
       "3  Road runner, road runner\\r\\nGoing hundred mile...  \n",
       "4  I don't need your forgiveness\\r\\nI don't need ...  \n",
       "5  One, two, three, go\\r\\n\\r\\nBroken\\r\\nYeah, you...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db0746",
   "metadata": {},
   "source": [
    "Creating a list of text columns for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5848fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['track','artist','lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5999e929",
   "metadata": {},
   "source": [
    "Importing packages for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d8919c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rohan\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rohan\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in c:\\users\\rohan\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\rohan\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rohan\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohan\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a431a33",
   "metadata": {},
   "source": [
    "Importing packages for stopword and punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c71ca8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import string\n",
    "punctuations=list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfec4d6",
   "metadata": {},
   "source": [
    "Importing packages for part of speech recognition and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d826590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Rohan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c5006",
   "metadata": {},
   "source": [
    "We will build a function that will tokenize the lyrics into words, remove stopwords and punctuations and also lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9a384eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined to tokenize text, remove stopwords and punctuation and lemmatize words\n",
    "def process_text(text):\n",
    "    # Word tokenization\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Lemmatization, Stopword and punctuation removal\n",
    "    words = [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word)) for word in words if word.lower() not in stop_words and word not in punctuations]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69b430",
   "metadata": {},
   "source": [
    "Let us apply the function to the three main textual features of our dataset. We will be using only these to try and predict a valence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "45eb1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in text_cols:\n",
    "    df_cleaned[col] = df_cleaned[col].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59fece",
   "metadata": {},
   "source": [
    "Let us have a look at the data after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "16c7698f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>number_of_emotion_tags</th>\n",
       "      <th>valence_tags</th>\n",
       "      <th>arousal_tags</th>\n",
       "      <th>dominance_tags</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[st., anger]</td>\n",
       "      <td>[metallica]</td>\n",
       "      <td>8</td>\n",
       "      <td>3.710000</td>\n",
       "      <td>5.833000</td>\n",
       "      <td>5.427250</td>\n",
       "      <td>[st., anger, 'round, neck, st., anger, 'round,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[speedin]</td>\n",
       "      <td>[rick, ross]</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>5.870000</td>\n",
       "      <td>5.490000</td>\n",
       "      <td>[legendary, runner, know, ooooh, ooooh, trilla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bamboo, banga]</td>\n",
       "      <td>[m.i.a]</td>\n",
       "      <td>13</td>\n",
       "      <td>6.555071</td>\n",
       "      <td>5.537214</td>\n",
       "      <td>5.691357</td>\n",
       "      <td>[road, runner, road, runner, go, hundred, mile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[die, mf, die]</td>\n",
       "      <td>[dope]</td>\n",
       "      <td>7</td>\n",
       "      <td>3.771176</td>\n",
       "      <td>5.348235</td>\n",
       "      <td>5.441765</td>\n",
       "      <td>[n't, need, forgiveness, n't, need, hate, n't,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[step]</td>\n",
       "      <td>[drown, pool]</td>\n",
       "      <td>9</td>\n",
       "      <td>2.971389</td>\n",
       "      <td>5.537500</td>\n",
       "      <td>4.726389</td>\n",
       "      <td>[one, two, three, go, broken, yeah, livin, edg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             track         artist  number_of_emotion_tags  valence_tags  \\\n",
       "1     [st., anger]    [metallica]                       8      3.710000   \n",
       "2        [speedin]   [rick, ross]                       1      3.080000   \n",
       "3  [bamboo, banga]        [m.i.a]                      13      6.555071   \n",
       "4   [die, mf, die]         [dope]                       7      3.771176   \n",
       "5           [step]  [drown, pool]                       9      2.971389   \n",
       "\n",
       "   arousal_tags  dominance_tags  \\\n",
       "1      5.833000        5.427250   \n",
       "2      5.870000        5.490000   \n",
       "3      5.537214        5.691357   \n",
       "4      5.348235        5.441765   \n",
       "5      5.537500        4.726389   \n",
       "\n",
       "                                              lyrics  \n",
       "1  [st., anger, 'round, neck, st., anger, 'round,...  \n",
       "2  [legendary, runner, know, ooooh, ooooh, trilla...  \n",
       "3  [road, runner, road, runner, go, hundred, mile...  \n",
       "4  [n't, need, forgiveness, n't, need, hate, n't,...  \n",
       "5  [one, two, three, go, broken, yeah, livin, edg...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92cc82",
   "metadata": {},
   "source": [
    "Let us now use word embedding to convert words into vectors so that we can use them to train models. We have used the twitter glove model as we believe that tweets can have both formal and casual/slang languages, which might be the case in lyrics of a song as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b619cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from pathlib import Path\n",
    "\n",
    "filepath = Path('bin/glove_twitter_50.d2v')\n",
    "\n",
    "if filepath.is_file():\n",
    "    w2vmodel = KeyedVectors.load(\"bin/glove_twitter_50.d2v\")\n",
    "else:\n",
    "    w2vmodel = api.load('glove-twitter-50')\n",
    "    w2vmodel.save('bin/glove_twitter_50.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102f1dc",
   "metadata": {},
   "source": [
    "Let us have a quick glimpse of how the model responds to contractions/slang words found in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "baa360c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stayin', 0.8281562328338623),\n",
       " ('nothin', 0.7884376049041748),\n",
       " ('lovin', 0.7804484963417053),\n",
       " ('thinkin', 0.7743610143661499),\n",
       " ('feelin', 0.7609580755233765),\n",
       " ('spendin', 0.7592039108276367),\n",
       " ('movin', 0.756058931350708),\n",
       " ('living', 0.7520089745521545),\n",
       " ('grindin', 0.7500482201576233),\n",
       " ('keepin', 0.744117796421051)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.most_similar('livin', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf2246",
   "metadata": {},
   "source": [
    "It returned pretty good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143ea960",
   "metadata": {},
   "source": [
    "Let us define a function to convert words to vectors. This function will be used on lyrics, artist name and track name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0781f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_w2v(words):\n",
    "    vocab_words = [word for word in words if word in w2vmodel.index_to_key]\n",
    "    if vocab_words:\n",
    "        vec = np.mean(w2vmodel[vocab_words], axis=0)\n",
    "    else:\n",
    "        vec = np.zeros(w2vmodel.vector_size)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42a49979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec = pd.DataFrame()\n",
    "for col in text_cols:\n",
    "    df_vec[col] = df_cleaned[col].apply(convert_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bee8b",
   "metadata": {},
   "source": [
    "Let us have a glimpse of the data after vector conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ed218fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.165115, 0.047045022, 0.011944994, -0.047220...</td>\n",
       "      <td>[-1.5121, 0.56089, 0.31079, 0.5117, -0.37686, ...</td>\n",
       "      <td>[-0.18695752, 0.09429084, 0.21492447, 0.041842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-2.5112, 0.2019, 0.41856, -0.51534, -0.26289,...</td>\n",
       "      <td>[-0.68313503, -0.41352, 0.458995, -0.807595, -...</td>\n",
       "      <td>[-0.13528791, 0.086075895, 0.00049746176, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.734945, -0.0005199909, 0.420065, -0.242645...</td>\n",
       "      <td>[-1.8619, 0.57832, -0.11964, -0.21855, -0.3286...</td>\n",
       "      <td>[-0.14192171, 0.22574994, 0.19820145, -0.42629...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.44762668, 1.00113, -0.2718633, -0.45729664,...</td>\n",
       "      <td>[-0.41516, -0.01654, -0.089277, -0.29688, 1.64...</td>\n",
       "      <td>[0.36944926, 0.4410963, -0.23828115, -0.190674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[-0.39883, -0.050333, 0.30032, -0.44223, 0.581...</td>\n",
       "      <td>[-0.99039, 0.443645, 0.663595, -0.284985, 0.00...</td>\n",
       "      <td>[0.11768429, 0.2919641, 0.32031235, -0.2069796...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               track  \\\n",
       "1  [0.165115, 0.047045022, 0.011944994, -0.047220...   \n",
       "2  [-2.5112, 0.2019, 0.41856, -0.51534, -0.26289,...   \n",
       "3  [-0.734945, -0.0005199909, 0.420065, -0.242645...   \n",
       "4  [0.44762668, 1.00113, -0.2718633, -0.45729664,...   \n",
       "5  [-0.39883, -0.050333, 0.30032, -0.44223, 0.581...   \n",
       "\n",
       "                                              artist  \\\n",
       "1  [-1.5121, 0.56089, 0.31079, 0.5117, -0.37686, ...   \n",
       "2  [-0.68313503, -0.41352, 0.458995, -0.807595, -...   \n",
       "3  [-1.8619, 0.57832, -0.11964, -0.21855, -0.3286...   \n",
       "4  [-0.41516, -0.01654, -0.089277, -0.29688, 1.64...   \n",
       "5  [-0.99039, 0.443645, 0.663595, -0.284985, 0.00...   \n",
       "\n",
       "                                              lyrics  \n",
       "1  [-0.18695752, 0.09429084, 0.21492447, 0.041842...  \n",
       "2  [-0.13528791, 0.086075895, 0.00049746176, -0.0...  \n",
       "3  [-0.14192171, 0.22574994, 0.19820145, -0.42629...  \n",
       "4  [0.36944926, 0.4410963, -0.23828115, -0.190674...  \n",
       "5  [0.11768429, 0.2919641, 0.32031235, -0.2069796...  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5153e5c",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dcf23dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c146c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(vec):\n",
    "    n_components = 30\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(vec)\n",
    "    pca_cols = ['PCA'+col+str(i) for i in range(1, n_components+1)]\n",
    "    df_ret = pd.DataFrame(data=pca_data, columns=pca_cols)\n",
    "    \n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8be20b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15085, 30)\n",
      "(15085, 30)\n",
      "(15085, 30)\n"
     ]
    }
   ],
   "source": [
    "df_pca = pd.DataFrame()\n",
    "for col in text_cols:\n",
    "    df_temp = pca_transform(df_vec[col].tolist())\n",
    "    df_pca = pd.concat([df_pca, df_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "94aad6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.drop(text_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b1db7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.reset_index(drop=True,inplace=True)\n",
    "df_cleaned.reset_index(drop=True,inplace=True)\n",
    "df_cleaned = pd.concat([df_cleaned,df_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e1e4db5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15085, 94)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f986945",
   "metadata": {},
   "source": [
    "Now we will move the dependent variable, i.e. valence tags in our case, to the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "dfe7b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_cleaned.pop('valence_tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d18ee5b",
   "metadata": {},
   "source": [
    "We will copy the remaining features to our X variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "57335efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680c6b5",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43b022",
   "metadata": {},
   "source": [
    "As we have a huge dataset, we have decided to use 70% for training and 30% for testing. The testing set is further divided into 60% for validation and 40% for final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e6b8d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cbaa4",
   "metadata": {},
   "source": [
    "We will download the split data into separate pickle files so that we can directly use them in our further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "77b27aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(X_train, 'Datasets/X_train.pkl')\n",
    "pd.to_pickle(X_val, 'Datasets/X_val.pkl')\n",
    "pd.to_pickle(X_test, 'Datasets/X_test.pkl')\n",
    "\n",
    "pd.to_pickle(y_train, 'Datasets/y_train.pkl')\n",
    "pd.to_pickle(y_val, 'Datasets/y_val.pkl')\n",
    "pd.to_pickle(y_test, 'Datasets/y_test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
